{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdmnVPTHk/2FonnMpkcILd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirin1996/Prediction_Project/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ub86MqcwyDM"
      },
      "source": [
        "import math  \n",
        "from typing import Tuple \n",
        "\n",
        "import torch \n",
        "from torch import nn, Tensor \n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import copy\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ise-XAft4pX2"
      },
      "source": [
        "##**Define the model**\n",
        "\n",
        "we train a `nn.TransformerEncoder` model on a language modeling task. The language modeling task is to assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words. A sequence of tokens are passed to the embedding layer first, followed by a positional encoding layer to account for the order of the word (see the next paragraph for more details). The `nn.TransformerEncoder` consists of multiple layers of `nn.TransformerEncoderLayer`. Along with the input sequence, a square attention mask is required because the self-attention layers in `nn.TransformerEncoder` are only allowed to attend the earlier positions in the sequence. For the language modeling task, any tokens on the future positions should be masked. To produce a probability distribution over output words, the output of the nn.`TransformerEncoder` model is passed through a linear layer followed by a log-softmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXDObxXaw922"
      },
      "source": [
        "class TransformerModel(nn.Module): #bulding transformer from scratch \n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__() #inheret from nn.Module \n",
        "        self.model_type = 'Transformer' \n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout) #applying PositionalEncoding to depict the order of words\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) #trasformer layers\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model) #input embedding \n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None: #weights of our model\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor: # the forward propogation function \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXAHcoB85o4c"
      },
      "source": [
        "`PositionalEncoding` module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use `sine`and `cosine` functions of different frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmHkuy2DyC4o"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggsmYoB6GV4"
      },
      "source": [
        "##**Load and batch data**\n",
        "\n",
        "Batching enables more parallelizable processing. However, batching means that the model treats each column independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWRC4CE59KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f91bd7f-c6c7-4934-f890-6d796b68c297"
      },
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 54.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1sfGZY_6q97"
      },
      "source": [
        "##**Functions to generate input and target sequence**\n",
        "\n",
        "`get_batch()` generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length `bptt`. For the language modeling task, the model needs the following words as `Target`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIEFmmRX6U0G"
      },
      "source": [
        "bptt = 10\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8_qFe0n686_"
      },
      "source": [
        "##**Initiate an instance**\n",
        "\n",
        "The model hyperparameters are defined below. The vocab size is equal to the length of the vocab object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRLifuEm64gK"
      },
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3eFLvX-7TLc"
      },
      "source": [
        "##**Run the model**\n",
        "We use `CrossEntropyLoss` with the *SGD (stochastic gradient descent)* optimizer. The learning rate is initially set to `5.0` and follows a StepLR schedule. During training, we use `nn.utils.clip_grad_norm_` to prevent gradients from exploding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMBfUQDQ7HLc"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        batch_size = data.size(0)\n",
        "        if batch_size != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm3A2A5P76nr"
      },
      "source": [
        "Finally loop over epochs. Save the model if the validation loss is the best we’ve seen so far. Adjust the learning rate after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMA_YkC271_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d28f72-6379-4994-bc6f-70da370049d9"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 1\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/10249 batches | lr 5.00 | ms/batch 230.34 | loss  8.09 | ppl  3255.99\n",
            "| epoch   1 |   400/10249 batches | lr 5.00 | ms/batch 225.21 | loss  6.88 | ppl   971.58\n",
            "| epoch   1 |   600/10249 batches | lr 5.00 | ms/batch 225.14 | loss  6.66 | ppl   784.01\n",
            "| epoch   1 |   800/10249 batches | lr 5.00 | ms/batch 228.21 | loss  6.49 | ppl   657.60\n",
            "| epoch   1 |  1000/10249 batches | lr 5.00 | ms/batch 232.36 | loss  6.46 | ppl   637.72\n",
            "| epoch   1 |  1200/10249 batches | lr 5.00 | ms/batch 235.91 | loss  6.38 | ppl   588.14\n",
            "| epoch   1 |  1400/10249 batches | lr 5.00 | ms/batch 237.78 | loss  6.36 | ppl   578.69\n",
            "| epoch   1 |  1600/10249 batches | lr 5.00 | ms/batch 239.41 | loss  6.23 | ppl   508.10\n",
            "| epoch   1 |  1800/10249 batches | lr 5.00 | ms/batch 238.17 | loss  6.30 | ppl   546.69\n",
            "| epoch   1 |  2000/10249 batches | lr 5.00 | ms/batch 240.66 | loss  6.18 | ppl   483.29\n",
            "| epoch   1 |  2200/10249 batches | lr 5.00 | ms/batch 236.47 | loss  6.24 | ppl   512.54\n",
            "| epoch   1 |  2400/10249 batches | lr 5.00 | ms/batch 237.00 | loss  6.23 | ppl   506.75\n",
            "| epoch   1 |  2600/10249 batches | lr 5.00 | ms/batch 235.88 | loss  6.18 | ppl   483.76\n",
            "| epoch   1 |  2800/10249 batches | lr 5.00 | ms/batch 235.62 | loss  6.24 | ppl   515.40\n",
            "| epoch   1 |  3000/10249 batches | lr 5.00 | ms/batch 235.74 | loss  6.18 | ppl   482.56\n",
            "| epoch   1 |  3200/10249 batches | lr 5.00 | ms/batch 233.04 | loss  6.22 | ppl   500.24\n",
            "| epoch   1 |  3400/10249 batches | lr 5.00 | ms/batch 232.03 | loss  6.18 | ppl   483.01\n",
            "| epoch   1 |  3600/10249 batches | lr 5.00 | ms/batch 231.69 | loss  6.28 | ppl   535.42\n",
            "| epoch   1 |  3800/10249 batches | lr 5.00 | ms/batch 231.01 | loss  6.19 | ppl   489.08\n",
            "| epoch   1 |  4000/10249 batches | lr 5.00 | ms/batch 231.84 | loss  6.20 | ppl   495.14\n",
            "| epoch   1 |  4200/10249 batches | lr 5.00 | ms/batch 229.51 | loss  6.28 | ppl   531.63\n",
            "| epoch   1 |  4400/10249 batches | lr 5.00 | ms/batch 233.89 | loss  6.24 | ppl   510.61\n",
            "| epoch   1 |  4600/10249 batches | lr 5.00 | ms/batch 231.37 | loss  6.24 | ppl   512.24\n",
            "| epoch   1 |  4800/10249 batches | lr 5.00 | ms/batch 230.37 | loss  6.16 | ppl   474.52\n",
            "| epoch   1 |  5000/10249 batches | lr 5.00 | ms/batch 230.27 | loss  6.24 | ppl   512.41\n",
            "| epoch   1 |  5200/10249 batches | lr 5.00 | ms/batch 230.39 | loss  6.25 | ppl   519.06\n",
            "| epoch   1 |  5400/10249 batches | lr 5.00 | ms/batch 227.85 | loss  6.24 | ppl   514.81\n",
            "| epoch   1 |  5600/10249 batches | lr 5.00 | ms/batch 228.83 | loss  6.23 | ppl   508.90\n",
            "| epoch   1 |  5800/10249 batches | lr 5.00 | ms/batch 227.76 | loss  6.19 | ppl   489.53\n",
            "| epoch   1 |  6000/10249 batches | lr 5.00 | ms/batch 227.85 | loss  6.17 | ppl   476.46\n",
            "| epoch   1 |  6200/10249 batches | lr 5.00 | ms/batch 228.58 | loss  6.20 | ppl   494.66\n",
            "| epoch   1 |  6400/10249 batches | lr 5.00 | ms/batch 228.26 | loss  6.28 | ppl   533.04\n",
            "| epoch   1 |  6600/10249 batches | lr 5.00 | ms/batch 227.43 | loss  6.22 | ppl   501.15\n",
            "| epoch   1 |  6800/10249 batches | lr 5.00 | ms/batch 226.47 | loss  6.22 | ppl   502.66\n",
            "| epoch   1 |  7000/10249 batches | lr 5.00 | ms/batch 226.41 | loss  6.20 | ppl   492.09\n",
            "| epoch   1 |  7200/10249 batches | lr 5.00 | ms/batch 226.87 | loss  6.17 | ppl   476.47\n",
            "| epoch   1 |  7400/10249 batches | lr 5.00 | ms/batch 225.90 | loss  6.12 | ppl   456.08\n",
            "| epoch   1 |  7600/10249 batches | lr 5.00 | ms/batch 225.39 | loss  6.12 | ppl   456.59\n",
            "| epoch   1 |  7800/10249 batches | lr 5.00 | ms/batch 225.12 | loss  6.18 | ppl   483.09\n",
            "| epoch   1 |  8000/10249 batches | lr 5.00 | ms/batch 226.39 | loss  6.22 | ppl   501.40\n",
            "| epoch   1 |  8200/10249 batches | lr 5.00 | ms/batch 226.99 | loss  6.21 | ppl   500.02\n",
            "| epoch   1 |  8400/10249 batches | lr 5.00 | ms/batch 226.42 | loss  6.19 | ppl   489.93\n",
            "| epoch   1 |  8600/10249 batches | lr 5.00 | ms/batch 227.45 | loss  6.23 | ppl   509.91\n",
            "| epoch   1 |  8800/10249 batches | lr 5.00 | ms/batch 225.96 | loss  6.24 | ppl   513.96\n",
            "| epoch   1 |  9000/10249 batches | lr 5.00 | ms/batch 224.73 | loss  6.21 | ppl   500.17\n",
            "| epoch   1 |  9200/10249 batches | lr 5.00 | ms/batch 225.27 | loss  6.13 | ppl   459.17\n",
            "| epoch   1 |  9400/10249 batches | lr 5.00 | ms/batch 224.94 | loss  6.18 | ppl   483.89\n",
            "| epoch   1 |  9600/10249 batches | lr 5.00 | ms/batch 223.82 | loss  6.17 | ppl   479.11\n",
            "| epoch   1 |  9800/10249 batches | lr 5.00 | ms/batch 224.33 | loss  6.19 | ppl   487.49\n",
            "| epoch   1 | 10000/10249 batches | lr 5.00 | ms/batch 223.78 | loss  6.13 | ppl   457.19\n",
            "| epoch   1 | 10200/10249 batches | lr 5.00 | ms/batch 226.25 | loss  6.10 | ppl   444.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2421.09s | valid loss  6.11 | valid ppl   449.88\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZwXDerE80L3"
      },
      "source": [
        "   *#updated upon feature request*\n",
        "#**Evaluation the best model on the test dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ybCA8xzwp2",
        "outputId": "00ffe4e3-4837-4d65-b152-97c8852ebd1c"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  6.03 | test ppl   417.20\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3juCvyB9DSL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}